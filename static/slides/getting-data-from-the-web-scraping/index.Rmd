---
title: "Getting data from the web: scraping"
author: "INFO 5940 <br /> Cornell University"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      highlightStyle: magula
      highlightLines: true
      highlightLanguage: r
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
# generate CSS file
library(xaringanthemer)
style_duo_accent(
  primary_color = "#B31B1B",
  secondary_color = "#F7F7F7",
  inverse_header_color = "#222222",
  black_color = "#222222",
  header_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  text_font_google = xaringanthemer::google_font("Atkinson Hyperlegible"),
  code_font_google = xaringanthemer::google_font("Source Code Pro"),
  base_font_size = "24px",
  code_font_size = "20px",
  # title_slide_background_image = "https://github.com/uc-dataviz/course-notes/raw/main/images/hexsticker.svg",
  # title_slide_background_size = "contain",
  # title_slide_background_position = "top",
  header_h1_font_size = "2rem",
  header_h2_font_size = "1.75rem",
  header_h3_font_size = "1.5rem",
  extra_css = list(
    "h1" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h2" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    "h3" = list(
      "margin-block-start" = "0.4rem",
      "margin-block-end" = "0.4rem"
    ),
    ".tiny" = list("font-size" = "70%"),
    ".small" = list("font-size" = "90%"),
    ".midi" = list("font-size" = "150%"),
    ".tiny .remark-code" = list("font-size" = "70%"),
    ".small .remark-code" = list("font-size" = "90%"),
    ".midi .remark-code" = list("font-size" = "150%"),
    ".large" = list("font-size" = "200%"),
    ".xlarge" = list("font-size" = "600%"),
    ".huge" = list(
      "font-size" = "400%",
      "font-family" = "'Montserrat', sans-serif",
      "font-weight" = "bold"
    ),
    ".hand" = list(
      "font-family" = "'Gochi Hand', cursive",
      "font-size" = "125%"
    ),
    ".task" = list(
      "padding-right" = "10px",
      "padding-left" = "10px",
      "padding-top" = "3px",
      "padding-bottom" = "3px",
      "margin-bottom" = "6px",
      "margin-top" = "6px",
      "border-left" = "solid 5px #F1DE67",
      "background-color" = "#F3D03E"
    ),
    ".pull-left" = list(
      "width" = "49%",
      "float" = "left"
    ),
    ".pull-right" = list(
      "width" = "49%",
      "float" = "right"
    ),
    ".pull-left-wide" = list(
      "width" = "70%",
      "float" = "left"
    ),
    ".pull-right-narrow" = list(
      "width" = "27%",
      "float" = "right"
    ),
    ".pull-left-narrow" = list(
      "width" = "27%",
      "float" = "left"
    ),
    ".pull-right-wide" = list(
      "width" = "70%",
      "float" = "right"
    ),
    ".blue" = list(color = "#2A9BB7"),
    ".purple" = list(color = "#a493ba"),
    ".yellow" = list(color = "#f1de67"),
    ".gray" = list(color = "#222222")
  )
)

source(here::here("R", "slide-opts.R"))
```

```{r pkgs, include = FALSE, cache = FALSE}
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE
)

library(tidyverse)
library(rvest)
library(lubridate)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = rcis::base_size))
```

## Web scraping

* Data on a website with no API
* Still want a programmatic, reproducible way to obtain data
* Ability to scrape depends on the quality of the website

---

class: inverse, middle

# HyperText Markup Language

---

## HTML

```{r echo = FALSE}
include_graphics(path = "http://imgs.xkcd.com/comics/tags.png")
```

.footnote[Source: [xkcd](https://xkcd.com/1144/)]

---

## Process of HTML

1. The web browser sends a request to the server that hosts the website
1. The server sends the browser an HTML document
1. The browser uses instructions in the HTML to render the website

---

## Components of HTML code

```html
<html>
  <head>
    <title>Title</title>
    <link rel="icon" type="icon" href="http://a" />
    <script src="https://c.js"></script>
  </head>
  <body>
    <div>
      <p>Click <b>here</b> now.</p>
      <span>Frozen</span>
    </div>
    <table style="width:100%">
      <tr>
        <td>Kristen</td>
        <td>Bell</td>
      </tr>
    </table>
  <img src="http://ia.media-imdb.com/images.png"/>
  </body>
</html>
```

---

## Components of HTML code

```html
<a href="http://github.com">GitHub</a>
```

* `<a></a>` - tag name
* `href` - attribute (argument)
* `"http://github.com"` - attribute (value)
* `GitHub` - content

---

## Nested structure of HTML

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

## Find the content "here"

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * <span style="color:red">**`b`**</span>
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

## HTML only

```{r echo = FALSE, out.width = "60%"}
include_graphics(path = "/img/shiny-css-none.png")
```

---

class: inverse, middle

# Cascading style sheets

---

## HTML + CSS

```{r echo = FALSE, out.width = "50%"}
include_graphics(path = "/img/shiny-css.png")
```

---

## CSS code

```css
span {
  color: #ffffff;
}

.num {
  color: #a8660d;
}

table.data {
  width: auto;
}

#firstname {
  background-color: yellow;
}
```

---

## CSS code

```html
<span class="bigname" id="shiny">Shiny</span>
```

* `<span></span>` - tag name
* `bigname` - class (optional)
* `shiny` - id (optional)

---

## CSS selectors

```css
span
```

```css
.bigname
```

```css
span.bigname
```

```css
#shiny
```

---

## CSS selectors

Prefix | Matches
-------|--------
none   | tag
.      | class
#      | id

> [CSS diner](http://flukeout.github.io)

---

## Find the CSS selector

.pull-left[

```html
<body>
    <table id="content">
        <tr class='name'>
            <td class='firstname'>
                Kurtis
            </td>
            <td class='lastname'>
                McCoy
            </td>
        </tr>
        <tr class='name'>
            <td class='firstname'>
                Leah
            </td>
            <td class='lastname'>
                Guerrero
            </td>
        </tr>
    </table>
</body>
```

]

.pull-right[

1. The entire table
1. Just the element containing first names

]

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

background-image: url("/img/webb-telescope-first-image.png")

.footnote[Source: [James Webb Space Telescope/NASA](https://webbtelescope.org/contents/media/images/2022/038/01G7JGTH21B5GN9VCYAHBXKSD1)]

---

## Scraping presidential statements

```{r echo = FALSE, out.width = "40%"}
include_graphics(path = "https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Dwight_D._Eisenhower%2C_official_photo_portrait%2C_May_29%2C_1959.jpg/613px-Dwight_D._Eisenhower%2C_official_photo_portrait%2C_May_29%2C_1959.jpg")
```

.footnote[[Space exploration](https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space+exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100)]

---

class: inverse, middle

# rvest for web scraping

---

## Using `rvest` to read HTML

1. Collect the HTML source code of a webpage
2. Read the HTML of the page
3. Select and keep certain elements of the page that are of interest

---

## Get the page

```{r get-statement}
dwight <- read_html(x = "https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration")
dwight
```

---

## Find page elements

`rvest` lets you find elements by

1. HTML tags
1. HTML attributes
1. CSS selectors

---

## Find `a` elements

```{r nodes, dependson = "get-statement"}
html_elements(x = dwight, css = "a")
```

---

## SelectorGadget

* GUI tool used to identify CSS selector combinations from a webpage
1. Read [here](https://rvest.tidyverse.org/articles/articles/selectorgadget.html)
1. Drag **SelectorGadget** link into your browser's bookmark bar

---

## Using SelectorGadget

1. Navigate to a webpage
1. Open the SelectorGadget bookmark
1. Click on the item to scrape
1. Click on yellow items you do not want to scrape
1. Click on additional items that you do want to scrape
1. Rinse and repeat until only the items you want to scrape are highlighted in yellow
1. Copy the selector to use with `html_elements()`

---

## Find the CSS selector

Use Selector Gadget to find the CSS selector for the document's *speaker*.

Then, modify an argument in `html_elements` to look for this more specific CSS selector.

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

## Find the CSS selector

```{r get-speaker, dependson = "get-statement"}
html_elements(x = dwight, css = ".diet-title a")
```

---

## Get attributes and text of elements

```{r get-speaker-text, dependson = "get-statement"}
# identify element with speaker name
speaker <- html_elements(dwight, ".diet-title a") %>% 
  html_text2() # Select text of element

speaker
```

---

## Get attributes and text of elements

```{r get-speaker-link, dependson = "get-statement"}
speaker_link <- html_elements(dwight, ".diet-title a") %>% 
  html_attr("href")

speaker_link
```

---

## Date of statement

```{r get-date, dependson = "get-statement"}
date <- html_elements(x = dwight, css = ".date-display-single") %>%
  html_text2() %>% # Grab element text
  mdy() # Format using lubridate
date
```

---

## Speaker name

```{r get-speaker-2, dependson = "get-statement"}
speaker <- html_elements(x = dwight, css = ".diet-title a") %>%
  html_text2()
speaker
```
    
---

## Title

```{r get-title, dependson = "get-statement"}
title <- html_elements(x = dwight, css = "h1") %>%
  html_text2()
title
```

---

## Text

```{r get-text, dependson = "get-statement"}
text <- html_elements(x = dwight, css = "div.field-docs-content") %>%
  html_text2()

# This is a long document, so let's just display the first 1,000 characters
text %>% str_sub(1, 1000) 
```
    
---

## Make a function

Make a function called `scrape_docs` that

- Accepts a URL of an individual document
- Scrapes the page
- Returns a data frame containing the document's
    - Date
    - Speaker
    - Title
    - Full text

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 7)
```

---

## Practice scraping data

1. Look up the cost of living for your hometown on [Sperling's Best Places](http://www.bestplaces.net/)
1. Extract it with `html_elements()` and `html_text2()`

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 4)
```

---

## Practice scraping data

```{r sterling}
sterling <- read_html("http://www.bestplaces.net/cost_of_living/city/virginia/sterling")

col <- html_elements(sterling, css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)")
html_text2(col)

# or use a piped operation
sterling %>%
  html_elements(css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)") %>%
  html_text2()
```

---

## Tables

```{r sterling-table, dependson = "sterling"}
tables <- html_elements(sterling, css = "table")

tables %>%
  # get the first table
  nth(1) %>%
  # convert to data frame
  html_table(header = TRUE)
```

---

## Extract climate statistics

> Extract the climate statistics of your hometown as a data frame with useful column names

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

## Extract climate statistics

```{r sterling-climate, collapse = TRUE}
sterling_climate <- read_html("http://www.bestplaces.net/climate/city/virginia/sterling")

climate <- html_elements(sterling_climate, css = "table")
html_table(climate, header = TRUE, fill = TRUE)[[1]]

sterling_climate %>%
  html_elements(css = "table") %>%
  nth(1) %>%
  html_table(header = TRUE)
```

---

## Random observations on scraping

* Make sure you've obtained only what you want
* If you are having trouble parsing, try selecting a smaller subset of the thing you are seeking
* Confirm that there is no R package and no API
