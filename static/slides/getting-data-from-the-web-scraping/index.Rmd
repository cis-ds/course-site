---
title: "Getting data from the web: scraping"
author: "[MACS 30500](https://cfss.uchicago.edu) <br /> University of Chicago"
output: rcfss::xaringan
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, htmltools.preserve.raw = FALSE)
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.retina = 2, fig.width = 12
)

library(tidyverse)
library(rvest)
library(lubridate)
library(countdown)

set.seed(1234)
theme_set(theme_minimal(base_size = rcfss::base_size))
```

# Web scraping

* Data on a website with no API
* Still want a programatic, reproducible way to obtain data
* Ability to scrape depends on the quality of the website

---

# HTML

.center[

![[tags](https://xkcd.com/1144/)](http://imgs.xkcd.com/comics/tags.png)

]

---

# Process of HTML

1. The web browser sends a request to the server that hosts the website
1. The server sends the browser an HTML document
1. The browser uses instructions in the HTML to render the website

---

# Components of HTML code

```html
<html>
  <head>
    <title>Title</title>
    <link rel="icon" type="icon" href="http://a" />
    <script src="https://c.js"></script>
  </head>
  <body>
    <div>
      <p>Click <b>here</b> now.</p>
      <span>Frozen</span>
    </div>
    <table style="width:100%">
      <tr>
        <td>Kristen</td>
        <td>Bell</td>
      </tr>
    </table>
  <img src="http://ia.media-imdb.com/images.png"/>
  </body>
</html>
```

---

# Components of HTML code

```html
<a href="http://github.com">GitHub</a>
```

* `<a></a>` - tag name
* `href` - attribute (argument)
* `"http://github.com"` - attribute (value)
* `GitHub` - content

---

# Nested structure of HTML

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * `b`
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

# Find the content "here"

* `html`
    * `head`
        * `title`
        * `link`
        * `script`
    * `body`
        * `div`
            * `p`
                * <span style="color:red">**`b`**</span>
            * `span`
        * `table`
            * `tr`
                * `td`
                * `td`
        * `img`

---

# HTML only

![HTML only](/img/shiny-css-none.png)

---

# HTML + CSS

![HTML + CSS](/img/shiny-css.png)

---

# CSS code

```css
span {
  color: #ffffff;
}

.num {
  color: #a8660d;
}

table.data {
  width: auto;
}

#firstname {
  background-color: yellow;
}
```

---

# CSS code

```html
<span class="bigname" id="shiny">Shiny</span>
```

* `<span></span>` - tag name
* `bigname` - class (optional)
* `shiny` - id (optional)

---

# CSS selectors

```css
span
```

```css
.bigname
```

```css
span.bigname
```

```css
#shiny
```

---

# CSS selectors

Prefix | Matches
-------|--------
none   | tag
.      | class
#      | id

> [CSS diner](http://flukeout.github.io)

---

# Find the CSS selector

```html
<body>
    <table id="content">
        <tr class='name'>
            <td class='firstname'>
                Kurtis
            </td>
            <td class='lastname'>
                McCoy
            </td>
        </tr>
        <tr class='name'>
            <td class='firstname'>
                Leah
            </td>
            <td class='lastname'>
                Guerrero
            </td>
        </tr>
    </table>
</body>
```

1. The entire table
1. Just the element containing first names

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

# Scraping presidential statements

.center[

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Dwight_D._Eisenhower%2C_official_photo_portrait%2C_May_29%2C_1959.jpg/613px-Dwight_D._Eisenhower%2C_official_photo_portrait%2C_May_29%2C_1959.jpg)](https://www.presidency.ucsb.edu/advanced-search?field-keywords=%22space+exploration%22&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=&items_per_page=100)

]

---

# Using `rvest` to read HTML

1. Collect the HTML source code of a webpage
2. Read the HTML of the page
3. Select and keep certain elements of the page that are of interest

---

# Get the page

```{r get-statement}
dwight <- read_html(x = "https://www.presidency.ucsb.edu/documents/special-message-the-congress-relative-space-science-and-exploration")
dwight
```

---

# Find page elements

`rvest` lets you find elements by

1. HTML tags
1. HTML attributes
1. CSS selectors

---

# Find `a` elements

```{r nodes, dependson = "get-statement"}
html_elements(x = dwight, css = "a")
```

---

# Find the CSS selector

Use Selector Gadget to find the CSS selector for the document's *speaker*.

Then, modify an argument in `html_elements` to look for this more specific CSS selector.

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

# Find the CSS selector

```{r get-speaker, dependson = "get-statement"}
html_elements(x = dwight, css = ".diet-title a")
```

---

# Get attributes and text of elements

```{r get-speaker-text, dependson = "get-statement"}
# identify element with speaker name
speaker <- html_elements(dwight, ".diet-title a") %>% 
  html_text2() # Select text of element

speaker
```

---

# Get attributes and text of elements

```{r get-speaker-link, dependson = "get-statement"}
speaker_link <- html_elements(dwight, ".diet-title a") %>% 
  html_attr("href")

speaker_link
```

---

# Date of statement

```{r get-date, dependson = "get-statement"}
date <- html_elements(x = dwight, css = ".date-display-single") %>%
  html_text2() %>% # Grab element text
  mdy() # Format using lubridate
date
```

---

# Speaker name

```{r get-speaker-2, dependson = "get-statement"}
speaker <- html_elements(x = dwight, css = ".diet-title a") %>%
  html_text2()
speaker
```
    
---

# Title

```{r get-title, dependson = "get-statement"}
title <- html_elements(x = dwight, css = "h1") %>%
  html_text2()
title
```

---

# Text

```{r get-text, dependson = "get-statement"}
text <- html_elements(x = dwight, css = "div.field-docs-content") %>%
  html_text2()

# This is a long document, so let's just display the first 1,000 characters
text %>% str_sub(1, 1000) 
```
    
---

# Make a function

Make a function called `scrape_docs` that

- Accepts a URL of an individual document
- Scrapes the page
- Returns a data frame containing the document's
    - Date
    - Speaker
    - Title
    - Full text

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

# Practice scraping data

1. Look up the cost of living for your hometown on [Sperling's Best Places](http://www.bestplaces.net/)
1. Extract it with `html_elements()` and `html_text2()`

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 4)
```

---

# Practice scraping data

```{r sterling}
sterling <- read_html("http://www.bestplaces.net/cost_of_living/city/virginia/sterling")

col <- html_elements(sterling, css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)")
html_text2(col)

# or use a piped operation
sterling %>%
  html_elements(css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)") %>%
  html_text2()
```

---

# Tables

```{r sterling-table, dependson = "sterling"}
tables <- html_elements(sterling, css = "table")

tables %>%
  # get the first table
  nth(1) %>%
  # convert to data frame
  html_table(header = TRUE)
```

---

# Extract climate statistics

> Extract the climate statistics of your hometown as a data frame with useful column names

```{r echo = FALSE, cache = FALSE}
countdown(minutes = 3)
```

---

# Extract climate statistics

```{r sterling-climate, collapse = TRUE}
sterling_climate <- read_html("http://www.bestplaces.net/climate/city/virginia/sterling")

climate <- html_elements(sterling_climate, css = "table")
html_table(climate, header = TRUE, fill = TRUE)[[1]]

sterling_climate %>%
  html_elements(css = "table") %>%
  nth(1) %>%
  html_table(header = TRUE)
```

---

# Random observations on scraping

* Make sure you've obtained only what you want
* If you are having trouble parsing, try selecting a smaller subset of the thing you are seeking
* Confirm that there is no R package and no API
