---
title: "Diagnostic tests for OLS"
author: |
  | MACS 30200
  | University of Chicago
date: "May 3, 2017"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)

library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(titanic)
library(rcfss)
library(haven)
library(car)
library(lmtest)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal(base_size = 22))
```

## Assumptions of linear regression

$$Y_i = \alpha + \beta x_i + \epsilon_i$$

## Linearity

$$E(\epsilon_i) \equiv E(\epsilon | x_i) = 0$$

## Constant variance

$$V(\epsilon | x_i) = \sigma_{\epsilon}^2$$

## Normality

$$\epsilon \sim N(0, \sigma_\epsilon^2)$$

## Independence

* Any pair of errors $\epsilon_i$ and $\epsilon_j$ are independent for $i \neq j$

## $X$

* $X$ is assumed to be fixed
* $X$ is measured without error and independent of the error

    $$\epsilon_i \sim N(0, \sigma_\epsilon^2), \text{for } i = 1, \dots, n$$

## $X$ is not invariant

```{r invariant}
data_frame(x = 1,
           y = rnorm(10)) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "You cannot regress this",
       subtitle = "Slope is undefined")
```

## Handling violations of assumptions

* Makes inference difficult
* Move to robust inferential method
    * Nonparametric regression
    * Decision trees
    * Support vector machines
* Diagnose assumption violations and impose solutions within linear regression framework

## Unusual and influential data

* Outlier
* Leverage
* Discrepancy
* Influence

    $$\text{Influence} = \text{Leverage} \times \text{Discrepancy}$$

## Unusual and influential data

```{r flintstones-sim}
flintstones <- tribble(
  ~name,    ~x, ~y,
  "Barney", 13, 75,
  "Dino",   24, 300,
  "Betty",  14, 250,
  "Fred",   10, 220,
  "Wilma",  8,  210
)

ggplot(flintstones, aes(x, y, label = name)) +
  geom_smooth(data = filter(flintstones, name %in% c("Wilma", "Fred", "Betty")),
              method = "lm", se = FALSE, fullrange = TRUE, color = "gray",
              aes(linetype = "Betty + Fred + Wilma")) +
  geom_smooth(data = filter(flintstones, name != "Dino"),
              method = "lm", se = FALSE, fullrange = TRUE, color = "gray",
              aes(linetype = "Barney + Betty + Fred + Wilma")) +
  geom_smooth(data = filter(flintstones, name != "Barney"),
              method = "lm", se = FALSE, fullrange = TRUE, color = "gray",
              aes(linetype = "Betty + Dino + Fred + Wilma")) +
  scale_linetype_manual(values = c(3,2,1),
                        guide = guide_legend(nrow = 3,
                                             reverse = TRUE)) +
  geom_point(size = 2) +
  ggrepel::geom_label_repel() +
  labs(linetype = NULL) +
  theme(legend.position = "bottom")
```

## Measuring leverage

$$h_i = \frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{i'=1}^{n} (X_{i'} - \bar{X})^2}$$

$$h_i = \mathbf{X}_i (\mathbf{X'X})^{-1} \mathbf{X'}_i$$

* It is solely a function of $X$
* Larger values indicate higher leverage
* $\frac{1}{n} \leq h_i \leq 1$
* $\bar{h} = \frac{(p + 1)}{n}$

## Measuring discrepancy {.scrollable}

* Residuals
    
    $$V(E_i) = \sigma_\epsilon^2 (1 - h_i)$$

* Standardized residuals

    $$E'_i \equiv \frac{E_i}{S_{E} \sqrt{1 - h_i}}$$

    $$S_E = \sqrt{\frac{E_i^2}{(n - k - 1)}}$$

* Studentized residuals

    $$E_i^{\ast} \equiv \frac{E_i}{S_{E(-i)} \sqrt{1 - h_i}}$$

## Measuring influence

* $\text{DFBETA}_{ij}$

    $$D_{ij} = \hat{\beta_j} - \hat{\beta}_{j(-i)}, \text{for } i=1, \dots, n \text{ and } j = 0, \dots, k$$

* $\text{DFBETAS}_{ij}$

    $$D^{\ast}_{ij} = \frac{D_{ij}}{SE_{-i}(\beta_j)}$$

* Cook's $D$

    $$D_i = \frac{E^{'2}_i}{k + 1} \times \frac{h_i}{1 - h_i}$$

## Visualizing outliers

* Outcome of interest - number of federal laws struck down by SCOTUS
1. **Age** - the mean age of the members of the Supreme Court
1. **Tenure** - mean tenure of the members of the Court
1. **Unified** - a dummy variable indicating whether or not the Congress was controlled by the same party in that period

## Visualizing outliers

```{r dahl}
# read in data and estimate model
dahl <- read_dta("../data/LittleDahl.dta")
dahl_mod <- lm(nulls ~ age + tenure + unified, data = dahl)
tidy(dahl_mod)
```

## Visualizing outliers {.scrollable}

```{r dahl-time}
dahl <- dahl %>%
  mutate(year = congress * 2 + 1787)

ggplot(dahl, aes(year, nulls)) +
  geom_line() +
  geom_vline(xintercept = 1935, linetype = 2) +
  labs(x = "Year",
       y = "Congressional laws struck down")

ggplot(dahl, aes(year, age)) +
  geom_line() +
  geom_vline(xintercept = 1935, linetype = 2) +
  labs(x = "Year",
       y = "Mean age of justices on the Court")
```

## Visualizing outliers

```{r bubble}
# add key statistics
dahl_augment <- dahl %>%
  mutate(hat = hatvalues(dahl_mod),
         student = rstudent(dahl_mod),
         cooksd = cooks.distance(dahl_mod))

# draw bubble plot
ggplot(dahl_augment, aes(hat, student)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_point(aes(size = cooksd), shape = 1) +
  geom_text(data = dahl_augment %>%
              arrange(-cooksd) %>%
              slice(1:10),
            aes(label = Congress)) +
  scale_size_continuous(range = c(1, 20)) +
  labs(x = "Leverage",
       y = "Studentized residual") +
  theme(legend.position = "none")
```

## Hat values

> Anything exceeding twice the average $\bar{h}$ is noteworthy

```{r hat-sig}
dahl_augment %>%
  filter(hat > 2 * mean(hat))
```

## Studentized residuals

> Anything outside of the range $[-2,2]$ is discrepant

```{r resid-sig}
dahl_augment %>%
  filter(abs(student) > 2)
```

## Influence

> $$D_i > \frac{4}{n - k - 1}$$

```{r cooksd-sig}
dahl_augment %>%
  filter(cooksd > 4 / (nrow(.) - (length(coef(dahl_mod)) - 1) - 1))
```

## How to treat unusual observations

* Mistake
* Weird observation
    * Something unusual happened to that observation
    * No apparent reason
* To drop or not to drop

## How to treat unusual observations {.scrollable}

```{r dahl-reestimate}
dahl_omit <- dahl %>%
  filter(!(congress %in% c(74, 98, 104)))

dahl_omit_mod <- lm(nulls ~ age + tenure + unified, data = dahl_omit)

coefplot::multiplot(dahl_mod, dahl_omit_mod,
                    names = c("All observations",
                              "Omit outliers")) +
  theme(legend.position = "bottom")

# rsquared values
rsquare(dahl_mod, dahl)
rsquare(dahl_omit_mod, dahl_omit)

# rmse values
rmse(dahl_mod, dahl)
rmse(dahl_omit_mod, dahl_omit)
```


## Non-normally distributed errors

$$\epsilon \sim N(0, \sigma_\epsilon^2)$$

## Quantile comparison plot {.scrollable}

```{r slid}
(slid <- read_tsv("http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-3E/datasets/SLID-Ontario.txt"))

slid_mod <- lm(compositeHourlyWages ~ sex + yearsEducation + age, data = slid)
tidy(slid_mod)

car::qqPlot(slid_mod)
```

## Density plot

```{r slid-density}
augment(slid_mod, slid) %>%
  mutate(.student = rstudent(slid_mod)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5) +
  labs(x = "Studentized residuals",
       y = "Estimated density")
```

## Fixing non-normally distributed errors {.scrollable}

* Power and log transformations

```{r slid-log}
slid <- slid %>%
  mutate(wage_log = log(compositeHourlyWages))

slid_log_mod <- lm(wage_log ~ sex + yearsEducation + age, data = slid)
tidy(slid_log_mod)

car::qqPlot(slid_log_mod)

augment(slid_log_mod, slid) %>%
  mutate(.student = rstudent(slid_log_mod)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5) +
  labs(x = "Studentized residuals",
       y = "Estimated density")
```

## Non-constant error variance

$$\text{Var}(\epsilon_i) = \sigma^2$$

* Homoscedasticity
* Heteroscedasticity

## Detecting heteroscedasticity

```{r sim-homo}
sim_homo <- data_frame(x = runif(1000, 0, 10),
                       y = 2 + 3 * x + rnorm(1000, 0, 1))
sim_homo_mod <- glm(y ~ x, data = sim_homo)

sim_homo %>%
  add_predictions(sim_homo_mod) %>%
  add_residuals(sim_homo_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Homoscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

## Detecting heteroscedasticity

```{r sim-hetero}
sim_hetero <- data_frame(x = runif(1000, 0, 10),
                       y = 2 + 3 * x + rnorm(1000, 0, (x / 2)))
sim_hetero_mod <- glm(y ~ x, data = sim_hetero)

sim_hetero %>%
  add_predictions(sim_hetero_mod) %>%
  add_residuals(sim_hetero_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Heteroscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")
```

## Breusch-Pagan test

* Estimate an OLS model and obtain the squared residuals $\hat{\epsilon}^2$
* Regress $\hat{\epsilon}^2$ against all the $k$ variables you think might be causing the heteroscedasticity
* Calculate ($n R^2_{\hat{\epsilon}^2}$)
    * $\chi^2_{(k-1)}$ distribution
    * Rejecting the null hypothesis indicates heteroscedasticity is present

## Breusch-Pagan test

```{r breusch-pagan}
bptest(sim_homo_mod)
bptest(sim_hetero_mod)
```

## Weighted least squares regression

$$
\begin{bmatrix}
    \sigma_1^2       & 0 & 0 & 0 \\
    0       & \sigma_2^2 & 0 & 0 \\
    0       & 0 & \ddots & 0 \\
    0       & 0 & 0 & \sigma_n^2 \\
\end{bmatrix}
$$

## Weighted least squares regression

$$
\mathbf{W} =
\begin{bmatrix}
    \frac{1}{\sigma_1^2}       & 0 & 0 & 0 \\
    0       & \frac{1}{\sigma_2^2} & 0 & 0 \\
    0       & 0 & \ddots & 0 \\
    0       & 0 & 0 & \frac{1}{\sigma_n^2} \\
\end{bmatrix}
$$

## Weighted least squares regression

$$\hat{\mathbf{\beta}} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y}$$

$$\hat{\mathbf{\beta}} = (\mathbf{X}' \mathbf{W} \mathbf{X})^{-1} \mathbf{X}' \mathbf{W} \mathbf{y}$$

$$\sigma_{\epsilon}^2 = \frac{\sum(w_i E_i^2)}{n}$$

## Estimating the weights

1. Use the residuals from a preliminary OLS regression to obtain estimates of the error variance
1. Model the weights as a function of observable variables in the model

## Estimating the weights {.scrollable}

```{r wls}
# do we have heteroscedasticity?
slid_mod <- glm(compositeHourlyWages ~ sex + yearsEducation + age, data = slid)

slid %>%
  add_predictions(slid_mod) %>%
  add_residuals(slid_mod) %>%
  ggplot(aes(pred, resid)) +
  geom_point(alpha = .2) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_quantile(method = "rqss", lambda = 5, quantiles = c(.05, .95)) +
  labs(title = "Heteroscedastic variance of error terms",
       x = "Predicted values",
       y = "Residuals")

# convert residuals to weights
weights <- 1 / residuals(slid_mod)^2

slid_wls <- lm(compositeHourlyWages ~ sex + yearsEducation + age, data = slid, weights = weights)

tidy(slid_mod)
tidy(slid_wls)
```

## Corrections for the variance-covariance estimates

* Standard errors
* Huber-White standard errors

## Corrections for the variance-covariance estimates

```{r huber-white}
hw_std_err <- hccm(slid_mod, type = "hc1") %>%
  diag %>%
  sqrt

tidy(slid_mod) %>%
  mutate(std.error.rob = hw_std_err)
```

## Non-linearity in the data

$$\epsilon \sim N(0, \sigma_\epsilon^2)$$

* The relationship between $X_1$ and $Y$ is nonlinear
* The relationship between $X_1$ and $Y$ is conditional on $X_2$

## Partial residual plots

$$E_i^{(j)} = E_i + B_j X_{ij}$$

## Partial residual plots {.scrollable}

```{r part-resid-plot}
# get partial resids
slid_resid <- residuals(slid_log_mod, type = "partial") %>%
  as_tibble
names(slid_resid) <- str_c(names(slid_resid), "_resid")

slid_diag <- augment(slid_log_mod, slid) %>%
  bind_cols(slid_resid)

ggplot(slid_diag, aes(age, age_resid)) +
  geom_point(alpha = .1) +
  geom_smooth(se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  labs(x = "Age",
       y = "Partial residual for age")

ggplot(slid_diag, aes(yearsEducation, yearsEducation_resid)) +
  geom_point(alpha = .1) +
  geom_smooth(se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  labs(x = "Education (years)",
       y = "Partial residual for education")
```

## Correcting for nonlinearity

$$\log(\text{Wage}) = \beta_0 + \beta_1(\text{Male}) + \beta_2 \text{Age} + \beta_3 \text{Age}^2 + \beta_4 \text{Education}^2$$

```{r slid-part-transform}
slid_log_trans <- lm(wage_log ~ sex + I(yearsEducation^2) + age + I(age^2), data = slid)
tidy(slid_log_trans)
```

## Correcting for nonlinearity

* Partial residuals

    $$E_i^{\text{Age}} = `r coef(slid_log_trans)[[4]]` \times \text{Age}_i `r formatC(coef(slid_log_trans)[[5]])` \times \text{Age}^2_i + E_i$$

    $$E_i^{\text{Education}} = `r coef(slid_log_trans)[[3]]` \times \text{Education}^2_i + E_i$$

* Partial fits

    $$\hat{Y}_i^{(\text{Age})} = `r coef(slid_log_trans)[[4]]` \times \text{Age}_i `r formatC(coef(slid_log_trans)[[5]])` \times \text{Age}^2_i$$

    $$\hat{Y}_i^{(\text{Education})} = `r coef(slid_log_trans)[[3]]` \times \text{Education}^2_i$$

## Correcting for nonlinearity {.scrollable}

```{r slid-part-trans-plot}
# get partial resids
slid_trans_resid <- residuals(slid_log_trans, type = "partial") %>%
  as_tibble
names(slid_trans_resid) <- c("sex", "education", "age", "age_sq")
names(slid_trans_resid) <- str_c(names(slid_trans_resid), "_resid")

slid_trans_diag <- augment(slid_log_trans, slid) %>%
  as_tibble %>%
  mutate(age_resid = coef(slid_log_trans)[[4]] * age +
           coef(slid_log_trans)[[5]] * age^2,
         educ_resid = coef(slid_log_trans)[[5]] * yearsEducation^2)

ggplot(slid_trans_diag, aes(age, age_resid + .resid)) +
  geom_point(alpha = .1) +
  geom_smooth(aes(y = age_resid), se = FALSE) +
  geom_smooth(se = FALSE, linetype = 2) +
  labs(x = "Age",
       y = "Partial residual for age")

ggplot(slid_trans_diag, aes(yearsEducation, educ_resid + .resid)) +
  geom_point(alpha = .1) +
  geom_smooth(aes(y = educ_resid), se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, linetype = 2) +
  labs(x = "Education (years)",
       y = "Partial residual for education")
```

## Collinearity

> Where explanatory variables are correlated with one another

## Perfect collinearity

```{r mtcars}
mtcars1 <- lm(mpg ~ disp + wt + cyl, data = mtcars)
summary(mtcars1)
```

## Perfect collinearity

```{r mtcars-recenter}
mtcars <- mtcars %>%
  mutate(disp_mean = disp - mean(disp))

mtcars2 <- lm(mpg ~ disp + wt + cyl + disp_mean, data = mtcars)
summary(mtcars2)
```

## Perfect collinearity

```{r mtcars-cor}
ggplot(mtcars, aes(disp, disp_mean)) +
  geom_point()
```

## Less-than-perfect collinearity

```{r credit}
credit <- read_csv("../data/Credit.csv") %>%
  select(-X1)
names(credit) <- tolower(names(credit))

ggplot(credit, aes(limit, age)) +
  geom_point()
```

## Less-than-perfect collinearity

```{r credit-lm}
age_limit <- lm(balance ~ age + limit, data = credit)
tidy(age_limit)
```

## Less-than-perfect collinearity {.scrollable}

```{r add-rating}
ggplot(credit, aes(rating, balance)) +
  geom_point() +
  geom_smooth()

limit_rate <- lm(balance ~ limit + rating, data = credit)
tidy(limit_rate)

coefplot::multiplot(age_limit, limit_rate)
```

## Less-than-perfect collinearity {.scrollable}

```{r limit-rate}
ggplot(credit, aes(limit, rating)) +
  geom_point() +
  geom_smooth()

coefplot::multiplot(age_limit, limit_rate, predictors = "limit")
```

## Correlation matrix

```{r credit-cor-mat}
cormat_heatmap <- function(data){
  # generate correlation matrix
  cormat <- round(cor(data), 2)
  
  # melt into a tidy table
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
  upper_tri <- get_upper_tri(cormat)
  
  # reorder matrix based on coefficient value
  reorder_cormat <- function(cormat){
    # Use correlation between variables as distance
    dd <- as.dist((1-cormat)/2)
    hc <- hclust(dd)
    cormat <-cormat[hc$order, hc$order]
  }
  
  cormat <- reorder_cormat(cormat)
  upper_tri <- get_upper_tri(cormat)
  
  # Melt the correlation matrix
  melted_cormat <- reshape2::melt(upper_tri, na.rm = TRUE)
  
  # Create a ggheatmap
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  # add correlation values to graph
  ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.position = "bottom")
}

cormat_heatmap(select_if(credit, is.numeric))
```

## Scatterplot matrix

```{r credit-scatter-mat}
library(GGally)
ggpairs(select_if(credit, is.numeric))
```

Here it is very clear that limit and rating are strongly correlated with one another.

## Variance inflation factor (VIF)

* Ratio of the variance of $\hat{\beta}_j$ when fitting the full model divided by the variance of $\hat{\beta}_j$ if fit on its own model
* Rule of thumb - greater than 10

## Variance inflation factor (VIF)

```{r vif}
vif(age_limit)
vif(limit_rate)
```

## Fixing multicollinearity

* Drop one or more of the collinear variables from the model - NO!
* Add more data
* Transform the variables
* Shrinkage methods



