---
title: "Statistical learning: basics and regression problems"
author: |
  | MACS 30500
  | University of Chicago
date: "November 14, 2016"
output: rcfss::cfss_slides
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE,
                      interval = .4)

library(tidyverse)
library(modelr)
library(here)
theme_set(theme_bw(base_size = 18))
```

## What is statistical learning?

```{r get_ad, message = FALSE, warning = FALSE}
# get advertising data
advertising <- read_csv(here("data/Advertising.csv")) %>%
  tbl_df() %>%
  select(-X1)
```

```{r plot_ad, dependson="get_ad"}
# plot separate facets for relationship between ad spending and sales
plot_ad <- advertising %>%
  gather(method, spend, -Sales) %>%
  ggplot(aes(spend, Sales)) +
  facet_wrap(~ method, scales = "free_x") +
  geom_point() +
  labs(x = "Spending (in thousands of dollars)")
plot_ad
```

## Functional form

$$Y = f(X) + \epsilon$$

* Statistical learning refers to the set of approaches for estimating $f$

## Linear functional form

```{r plot_ad_fit}
plot_ad +
  geom_smooth(method = "lm", se = FALSE)
```

## Why estimate $f$?

* Prediction
* Inference
* How do we estimate $f$?
    * Parametric methods
    * Non-parametric methods

## Parametric methods

1. First make an assumption about the functional form of $f$
1. After a model has been selected, **fit** or **train** the model using the actual data

## OLS

```{r plot_parametric, dependson="get_ad"}
method_model <- function(df) {
  lm(Sales ~ spend, data = df)
}

ad_pred <- advertising %>%
  gather(method, spend, -Sales) %>%
  group_by(method) %>%
  nest() %>%
  mutate(model = map(data, method_model),
         pred = map(model, broom::augment)) %>%
  unnest(pred)

plot_ad +
  geom_smooth(method = "lm", se = FALSE) +
  geom_linerange(data = ad_pred,
                 aes(ymin = Sales, ymax = .fitted),
                 color = "blue",
                 alpha = .5) 
```

## Parametric methods

$$Y = \beta_0 + \beta_{1}X_1$$

* $Y =$ sales
* $X_{1} =$ advertising spending in a given medium
* $\beta_0 =$ intercept
* $\beta_1 =$ slope

## Non-parametric methods

* No assumptions about functional form
* Use data to estimate $f$ directly
    * Get close to data points
    * Avoid overcomplexity
* Requires large amount of observations

## LOESS

```{r loess, echo = FALSE, warning = FALSE, message = FALSE}
library(broom)
library(lattice)

mod <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
fit <- augment(mod)

ggplot(fit, aes(E, NOx)) +
  geom_point() +
  geom_line(aes(y = .fitted), color = "red") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")
```

----

```{r loess_buildup, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
dat <- ethanol %>%
  inflate(center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= .75) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

library(gganimate)

p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_line(aes(y = .fitted), data = fit, color = "red") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")
gg_animate(p)
```

----

```{r loess_span, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
spans <- c(.25, .5, .75, 1)

# create loess fits, one for each span
fits <- data_frame(span = spans) %>%
  group_by(span) %>%
  do(augment(loess(NOx ~ E, ethanol, degree = 1, span = .$span)))

# calculate weights to reproduce this with local weighted fits
dat <- ethanol %>%
  inflate(span = spans, center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= span) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

# create faceted plot with changing points, local linear fits, and vertical lines,
# and constant hollow points and loess fit
p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_point(shape = 1, data = ethanol, alpha = .25) +
  geom_line(aes(y = .fitted, frame = E, cumulative = TRUE), data = fits, color = "red") +
  facet_wrap(~span) +
  ylim(0, 5) +
  ggtitle("x0 = ") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")

gg_animate(p)
```

## Types of learning

* Supervised
* Unsupervised

## Statistical learning vs. machine learning

* Statistical learning
    * Subfield of statistics
    * Focused predominantly on **inference**
* Machine learning
    * Subfield of computer science
    * Focused predominantly on **prediction**

## Linear models

$$y = \beta_0 + \beta_1 * x$$

```{r sim-plot}
ggplot(sim1, aes(x, y)) + 
  geom_point()
```

## Linear models

```{r sim-random-fit}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

## Least squares regression

```{r sim-error, echo = FALSE}
dist1 <- sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, colour = "grey40") +
  geom_point(colour = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "#3366FF")
```

## Estimating a linear model using `lm()`

```{r sim-lm, echo = TRUE}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

## `str(lm())`

```{r lm-str, echo = TRUE}
str(sim1_mod)
```

## Predicted values

```{r add-predict-data}
grid <- sim1 %>% 
  data_grid(x) 
```

```{r add-predict}
grid <- grid %>% 
  add_predictions(sim1_mod) 
```

```{r plot-lm-predict}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1) +
  geom_point(aes(y = pred), data = grid, color = "blue", size = 3)
```

## Residuals

```{r resids}
sim1 <- sim1 %>% 
  add_residuals(sim1_mod)

ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
```

## Estimating linear models using `gapminder`

Go [here](stat002_linear_models.html#estimating_a_linear_model(s)_using_gapminder).

## Exercise: linear regression with `scorecard`

Complete [this exercise](stat002_linear_models.html#exercise:_linear_regression_with_scorecard)

## Acknowledgments

* This presentation is derived in part from ["Creating a LOESS animation with `gganimate`"](http://varianceexplained.org/files/loess.html) by David Robinson.
* This presentation is derived in part from ["Machine Learning vs. Statistical Modeling: The Real Difference"](http://www.infogix.com/blog/machine-learning-vs-statistical-modeling-the-real-difference/)
* For more information on statistical learning and the math behind these methods, see the awesome book [*An Introduction to Statistical Learning*](http://link.springer.com.proxy.uchicago.edu/book/10.1007%2F978-1-4614-7138-7)

